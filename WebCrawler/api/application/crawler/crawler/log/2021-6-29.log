2021-06-29 17:01:24 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 17:01:24 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 17:01:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 17:01:25 [scrapy.extensions.telnet] INFO: Telnet Password: 97c9e96abee45a9a
2021-06-29 17:01:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 17:01:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 17:01:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 17:01:25 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 17:01:25 [scrapy.core.engine] INFO: Spider opened
2021-06-29 17:01:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 17:01:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 17:01:25 [project_crawler] INFO: æ•°æ®å¤„ç†å¼€å§‹
2021-06-29 17:01:25 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:01:25 [scrapy.core.scraper] ERROR: Error processing {'title': '2021å¹´ä½ å¯èƒ½ä¸çŸ¥é“çš„ CSS ç‰¹æ€§ï¼ˆä¸‹ç¯‡ï¼‰', 'brief_content': 'åœ¨è¿™ä¸ªè¯é¢˜ä¸­ä¸»è¦æ•´ç†äº†æœ‰å…³äº CSS æ–¹é¢çš„ç‰¹æ€§ï¼Œå¹¶ä¸”å°½å¯èƒ½çš„æ•´ç†äº†ä¸€äº›å¤§å®¶ç°åœ¨èƒ½ç”¨æˆ–è¿‡ä¸äº†å¤šä¹…å°±èƒ½ç”¨çš„å±æ€§ã€‚å¦å¤–ï¼Œè™½ç„¶æ ‡é¢˜æ˜¯â€œæ–°ç‰¹æ€§â€ï¼Œä½†å…¶ä¸­æœ‰è›®å¤šç‰¹æ€§å¹¶ä¸æ˜¯â€œæ–°â€ï¼Œå¯èƒ½å·²ç»å‡ºç°åœ¨ä½ çš„é¡¹ç›®ä¸­', 'user_name': 'æ·˜ç³»å‰ç«¯å›¢é˜Ÿ', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'äº§å“ç»ç†ï¼šé¸¿è’™é‚£ä¸ªå¼€åœºåŠ¨ç”»æŒºå¸…çš„ ç»™å’±ä»¬é¡µé¢ä¹Ÿæ•´ä¸€ä¸ªå‘—', 'brief_content': 'æœ‰ä¸€å¤©å¼€ä¼šï¼Œäº§å“ç»ç†é—®ï¼šå¤§å®¶éƒ½å‡é¸¿è’™ç³»ç»Ÿäº†ä¹ˆï¼Ÿç´§æ¥ç€ä¸€ç¾¤äººç­”ï¼šæˆ‘ä»¬éƒ½ç”¨iPhoneâ€¦ å½“ç„¶å“ˆï¼Œæˆ‘è‡ªå·±ç”¨çš„æ˜¯å®‰å“ï¼Œä¸è¿‡ä¹Ÿä¸æ˜¯åä¸º(ç•™ä¸‹äº†æ²¡é’±çš„æ³ªæ°´)â€¦ å¬äº†ä»–è¿™ä¹ˆä¸€é—®æˆ‘è¿˜ä»¥ä¸ºè¿™æ˜¯è¦è®©æˆ‘ä»¬å¼€å‘é¸¿è’™åº”ç”¨äº†å‘¢', 'user_name': 'æ‰‹æ’•çº¢é»‘æ ‘', 'tags': [None, None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'ä¸‰åƒæ–‡å­—ï¼Œåªä¸ºå†™å¥½ä¸€ä¸ª Function.prototype.call', 'brief_content': 'Function.prototype.callï¼Œæ‰‹å†™ç³»åˆ—ï¼Œä¸‡æ–‡é¢è¯•ç³»åˆ—ï¼Œå¿…ä¼šç³»åˆ—å¿…åŒ…å«çš„å†…å®¹ï¼Œè¶³è§å…¶åœ¨å‰ç«¯çš„åˆ†é‡ã€‚ æœ¬æ–‡åŸºäºMDN å’Œ ECMA æ ‡å‡†ï¼Œå’Œå¤§å®¶ä¸€èµ·ä»æ–°è®¤è¯†callã€‚', 'user_name': 'äº‘çš„ä¸–ç•Œ', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'WOCï¼åŸæ¥ Linux ç»ˆç«¯ä¸‹å±…ç„¶è¿˜æœ‰è¿›ç¨‹è®°å¸åŠŸèƒ½ï¼Ÿï¼', 'brief_content': 'åœ¨ç³»ç»Ÿç®¡ç†ä¸­ï¼Œæœ‰æ—¶éœ€è¦è®°å½•ç”¨æˆ·å¯¹èµ„æºçš„æ¶ˆè´¹æƒ…å†µï¼Œä½œä¸ºå¯¹ç”¨æˆ·è´¦å·æ”¶å–è´¹ç”¨çš„ä¾æ®ã€‚è¿™äº›æ—¥å¿—ä¹Ÿå¯ä»¥ç”¨äºå®‰å…¨ç›®çš„ï¼Œæä¾›æœ‰å…³ç³»ç»Ÿæ´»åŠ¨çš„æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚', 'user_name': 'æ°å“¥çš„ITä¹‹æ—…', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'ä½¿ç”¨ vscode çœæ—¶çš„6ä¸ªæ’ä»¶', 'brief_content': 'ä½¿ç”¨ Visual Studio Code å¼€å‘é¡¹ç›®çœæ—¶çš„å…­ä¸ªæ’ä»¶ã€‚ 1. vscode-icon ä¸åŒçš„æ–‡ä»¶å±•ç¤ºä¸åŒçš„å›¾æ ‡ï¼Œæ–¹ä¾¿å¿«é€Ÿè¯†åˆ«æ–‡ä»¶ç±»å‹ã€‚ 2. Color Highlight é¢œè‰²æ ‡è®°ã€‚ ', 'user_name': 'Jimmy', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'ã€Œå¾®ä¿¡å°ç¨‹åºã€ç”Ÿæˆæ°´å°åŸç†ä¸æ’ä»¶ç¼–å†™', 'brief_content': 'ä¸€ å‰è¨€ ä»Šå¤©åˆ†äº«ä¸€ä¸ªå°ç¨‹åºç”Ÿæˆæ°´å°çš„å°æŠ€å·§â€”â€”canvasç»˜åˆ¶èƒŒæ™¯å›¾ï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šè¯¦ç»†ä»‹ç»ç»˜åˆ¶çš„ç»†èŠ‚ã€‚å¸Œæœ›å¼€å‘è¿‡å¾®ä¿¡å°ç¨‹åºçš„åŒå­¦å¯ä»¥æŠŠæ–‡ç« æ”¶è—èµ·æ¥ï¼Œè¿™æ ·å¦‚æœä»¥åé‡åˆ°ç±»ä¼¼çš„éœ€æ±‚ï¼Œå¯ä»¥ç¿»å‡ºæ¥ä½œä¸ºå‚è€ƒã€‚ æœ¬', 'user_name': 'æˆ‘ä¸æ˜¯å¤–æ˜Ÿäºº', 'tags': [None, None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'three.js å®ç°é£æš´äº‘ç‰¹æ•ˆ', 'brief_content': 'å¤§å®¶å¥½ï¼Œè¿™é‡Œæ˜¯ CSSå…¼WebGL é­”æ³•ä½¿â€”â€”alphardexã€‚ æœ¬æ–‡æˆ‘ä»¬å°†ç”¨three.jsæ¥å®ç°é£æš´äº‘ç‰¹æ•ˆï¼Œä¸€èµ·æ¥åˆ›ä½œå§ï¼', 'user_name': 'alphardex', 'tags': [None, None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'æ‰‹å†™ç³»åˆ—-å®ç°ä¸€ä¸ªé“‚é‡‘æ®µä½çš„ React', 'brief_content': 'æœ¬æ–‡å®ç°ç®€å•ç‰ˆæœ¬çš„ Reactï¼Œå‚è€ƒ React 16.8 çš„åŸºæœ¬åŠŸèƒ½ï¼ŒåŒ…æ‹¬è™šæ‹Ÿ DOMã€Fiberã€Diff ç®—æ³•ã€å‡½æ•°å¼ç»„ä»¶ã€hooks ç­‰ã€‚', 'user_name': 'æ¸…æ±¤é¥ºå­', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'æ¼«ç”» | å¦‚æœé¢è¯•æ—¶å¤§å®¶éƒ½è¯´çœŸè¯â€¦', 'brief_content': 'ç”»é¢è¿‡äºçœŸå®ï¼Œæ˜“å¼•èµ·ä¸é€‚è¯·æ…å…¥â€¦â€¦é¢è¯•é€ ç«ç®­ å…¥èŒæ‹§èºä¸ æ˜¯å½“å‰èŒåœºçš„ä¸­æœ€ç¾é£æ™¯çº¿ é¢è¯•æ—¶çš„é«˜æ ‡å‡†ã€ä¸¥è¦æ±‚ å®é™…å·¥ä½œå´æ˜¯ä¿®ä¿®è¡¥è¡¥æˆ–åšç€å¿™ç¢Œä½†æ— å…³ç´§è¦çš„äº‹æƒ… ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§ç°è±¡å‘¢ï¼Ÿ', 'user_name': 'è‹å—', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'èµ·è¯‰ä¹¦ï¼æ…•è¯¾ç½‘ä½ æ•¢æŠ„è¢­ï¼Œæˆ‘å°±æ•¢èµ·è¯‰ä½ ï¼', 'brief_content': 'è¿™å‘¨å°±å¼€å§‹èµ°èµ·è¯‰æµç¨‹å•¦ï¼æœ€è¿‘ä¹Ÿåœ¨çœ‹ã€Šä¸­å›½è‘—ä½œæƒæ³•ã€‹å’Œã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹ä¸­çš„ä¸€äº›æ³•å¾‹æ¡æ–‡ï¼Œæ­£å¸¸æƒ…å†µä¸‹ï¼Œä¸€ä¸ªç¨‹åºå‘˜ä¹Ÿä¸ä¼šå»çœ‹è¿™äº›æ³•å¾‹æ–‡ä»¶çš„ï¼Œä½†æ˜¯ç”Ÿæ´»ä¸­æ€»æœ‰æ„å¤–ã€‚', 'user_name': 'ç¨‹åºå‘˜åä¸‰', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'è¿˜åœ¨è¿Ÿç–‘æ˜¯å¦ä¸Štsï¼Ÿå…ˆä¸Šè½¦å†è¯´ï¼vue3+tså¼€å‘åˆä½“éªŒ', 'brief_content': 'æ–‡æœ¬ä¸»è¦ç»“åˆæ¡ˆä¾‹ä½“éªŒä¸€ä¸‹vue3+tså¼€å‘çš„å®é™…æ•ˆæœã€‚åˆ°åº•é€‚ä¸é€‚åˆä½ å’Œä½ çš„é¡¹ç›®ï¼Œè¿˜å¾—æ ¹æ®å„ä½çœ‹å®˜è‡ªå·±æŒæ¡ç¨‹åº¦å’Œé¡¹ç›®å®é™…æƒ…å†µç»¼åˆåˆ¤æ–­ã€‚', 'user_name': 'æ¨æ‘é•¿', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'ã€ŠBootstrap5é›¶åŸºç¡€åˆ°ç²¾é€šã€‹ç¬¬26èŠ‚ Bootstrap5æ¨¡æ€å¼¹æ¡†Modalç»„ä»¶ç”¨æ³•', 'brief_content': 'è¿™æ˜¯æˆ‘å‚ä¸æ›´æ–‡æŒ‘æˆ˜çš„ç¬¬26å¤©ï¼Œæ´»åŠ¨è¯¦æƒ…æŸ¥çœ‹ï¼š æ›´æ–‡æŒ‘æˆ˜ 26.1 Bootstrap5æ¨¡æ€å¼¹æ¡†å·¥ä½œåŸç† ä½¿ç”¨Bootstrapçš„JavaScriptæ¨¡å¼æ’ä»¶å°†å¯¹è¯æ¡†æ·»åŠ åˆ°ç«™ç‚¹ä¸­ï¼Œç”¨äºç¯ç®±ã€ç”¨æˆ·é€šçŸ¥æˆ–å®Œ', 'user_name': 'ä¿ºè€åˆ˜', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'ä¸‰å¤©ä¸‰å¤œï¼Œæ•´ç†äº†30å¼ é«˜æ¸…æ€ç»´å¯¼å›¾ | å¸¦ä½ é‡æ¸©ES6ï¼ˆä¸‹ï¼‰', 'brief_content': 'æœ€è¿‘é‡æ–°å­¦ä¹ ES6ï¼Œæˆ‘å°†æ ¸å¿ƒçŸ¥è¯†æ±‡æ€»æ•´ç†æˆè„‘å›¾~~å»ºè®®æ”¶è—ï¼Œè¯´ä¸å®šæŸå¤©è¦çœ‹ä¸€çœ¼ï¼ï¼~~~~~~~~~', 'user_name': 'LBJ', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'è§£æ”¾ç”Ÿäº§åŠ›ï¼Œè‡ªåŠ¨åŒ–ç”ŸæˆVueç»„ä»¶æ–‡æ¡£', 'brief_content': 'ä¸€ã€ç°çŠ¶ Vueæ¡†æ¶åœ¨å‰ç«¯å¼€å‘ä¸­åº”ç”¨å¹¿æ³›ï¼Œå½“ä¸€ä¸ªå¤šäººå¼€å‘çš„Vueé¡¹ç›®ç»è¿‡é•¿æœŸç»´æŠ¤ä¹‹åå¾€å¾€ä¼šæ²‰æ·€å‡ºå¾ˆå¤šçš„å…¬å…±ç»„ä»¶ï¼Œè¿™ä¸ªæ—¶å€™ç»å¸¸ä¼šå‡ºç°ä¸€ä¸ªäºº å¼€å‘äº†ä¸€ä¸ªç»„ä»¶è€Œå…¶ä»–ç»´æŠ¤è€…æˆ–æ–°æ¥æ‰‹çš„äººå´ä¸çŸ¥é“è¿™ä¸ªç»„ä»¶æ˜¯åšä»€ä¹ˆ', 'user_name': 'ä¸€åªèŠ±å–µ', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'ã€ŠBootstrap5é›¶åŸºç¡€åˆ°ç²¾é€šã€‹ç¬¬27èŠ‚ Bootstrap5å¼¹å‡ºæç¤ºå’Œå·¥å…·æç¤ºç»„ä»¶ç”¨æ³•', 'brief_content': 'è¿™æ˜¯æˆ‘å‚ä¸æ›´æ–‡æŒ‘æˆ˜çš„ç¬¬27å¤©ï¼Œæ´»åŠ¨è¯¦æƒ…æŸ¥çœ‹ï¼š æ›´æ–‡æŒ‘æˆ˜ 27.1 æ¦‚è¿° è¿™å‡ è¦è®²ä¸¤ä¸ªæ§ä»¶ï¼šå¼¹å‡ºæç¤ºï¼ˆPopoversï¼‰å’Œå·¥å…·æç¤ºï¼ˆTooltipsï¼‰ï¼Œè¿™ä¸¤ä¸ªç»„ä»¶åŠŸèƒ½éƒ½å¾ˆå•ä¸€ï¼Œç”¨æ³•ä¹Ÿå¾ˆç®€å•ï¼Œæœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹', 'user_name': 'ä¿ºè€åˆ˜', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'æœ‰è¶£çš„ Kotlin 0x01ï¼šScala-like functions', 'brief_content': '0x01ï¼šScala-like functions ä»¥ä¸Šä»£ç ï¼Œè¿è¡Œç»“æœä¸ºä½•ï¼Ÿå¯é€‰é¡¹ï¼š Does not compile Prints "Hello, World" Nothing Something ', 'user_name': 'æ˜“å†¬', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'ä¸€ä¸ªé«˜æ€§èƒ½ã€å°è€Œç¾çš„åºåˆ—åŒ–å·¥å…·ï¼', 'brief_content': 'ä½œè€…ï¼šfredalxin\\ åœ°å€ï¼šhttps://fredal.xin/kryo-quickstart Kryoæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„åºåˆ—åŒ–/ååºåˆ—åŒ–å·¥å…·ï¼Œç”±äºå…¶å˜é•¿å­˜å‚¨ç‰¹æ€§å¹¶ä½¿ç”¨äº†å­—èŠ‚ç ç”Ÿæˆæœºåˆ¶ï¼Œæ‹¥æœ‰è¾ƒ', 'user_name': 'JavaæŠ€æœ¯æ ˆ', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'å‰ç«¯åº”è¯¥æŒæ¡çš„ç¼–è¯‘åŸºç¡€ï¼ˆåŸºäº babelï¼‰', 'brief_content': 'å¼€å‘æ¯æ¯ç›¸å…³ è™½ç„¶ Babel å›¢é˜Ÿåœ¨å„ç§å“­ç©·ï¼Œä½†æ˜¯ Babel å§‹ç»ˆæ˜¯æˆ‘ä»¬å‰ç«¯åœ¨å¼€å‘ä¸­ä¸å¯æˆ–ç¼ºçš„é‡è¦å·¥å…·ã€‚ è™½ç„¶æˆ‘ä»¬åªæ˜¯ API è°ƒç”¨å·¥ï¼Œä½†æ˜¯å¤šäº†è§£ä¸€äº›æ€»æ˜¯ä¼šæœ‰å¥½å¤„çš„å˜› â˜„ï¸â˜„ï¸â˜„ï¸', 'user_name': 'é™ˆå¤§é±¼å¤´', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': '14ä¸‡å­— | 400 å¤šé“ JavaScript é¢è¯•é¢˜ ğŸ“ æœ‰ç­”æ¡ˆ ğŸŒ (ç¬¬äºŒéƒ¨åˆ† 101-200é¢˜)', 'brief_content': 'è¿™æ˜¯æˆ‘å‚ä¸æ›´æ–‡æŒ‘æˆ˜çš„ç¬¬28å¤©ï¼Œæ´»åŠ¨è¯¦æƒ…æŸ¥çœ‹ï¼š æ›´æ–‡æŒ‘æˆ˜ 14ä¸‡å­— | 400 å¤šé“ JavaScript é¢è¯•é¢˜ ğŸ“ æœ‰ç­”æ¡ˆ ğŸŒ (ç¬¬äºŒéƒ¨åˆ† 101-200é¢˜)', 'user_name': 'æµ·æ‹¥', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.core.scraper] ERROR: Error processing {'title': 'Django Rest Framework æºç è§£è¯»ï¼ˆ å…­ï¼‰åˆ†é¡µåŠŸèƒ½', 'brief_content': 'è¿™æ˜¯æˆ‘å‚ä¸æ›´æ–‡æŒ‘æˆ˜çš„ç¬¬18å¤©ï¼Œæ´»åŠ¨è¯¦æƒ…æŸ¥çœ‹ï¼š æ›´æ–‡æŒ‘æˆ˜ ä¸€ã€åˆ†é¡µä»‹ç» åˆ†é¡µæœ‰ä¸‰ç§æ–¹å¼ æ™®é€šåˆ†é¡µï¼Œçœ‹ç¬¬né¡µï¼Œæ¯é¡µæ˜¾ç¤ºmæ¡æ•°æ®ï¼› åˆ‡å‰²åˆ†é¡µï¼Œåœ¨nä¸ªä½ç½®ï¼Œå‘åæŸ¥çœ‹mæ¡æ•°æ®ï¼› åŠ å¯†åˆ†é¡µï¼Œè¿™ä¸æ™®é€šåˆ†é¡µæ–¹å¼ç›¸ä¼¼ï¼Œ', 'user_name': 'Honest1y', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:01:26 [project_crawler] INFO: æ•°æ®å¤„ç†ç»“æŸ
2021-06-29 17:01:26 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 17:01:26 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\JsonExporterPipleline.py", line 22, in close_spider
    self.log("çˆ¬è™«ç»“æŸ", logging.INFO)
AttributeError: 'JsonExporterPipleline' object has no attribute 'log'
2021-06-29 17:01:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 478,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 15598,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.755341,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 9, 1, 26, 127229),
 'log_count/ERROR': 24,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 9, 1, 25, 371888)}
2021-06-29 17:01:26 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 17:13:13 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 17:13:13 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 17:13:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 17:13:13 [scrapy.extensions.telnet] INFO: Telnet Password: 9e51a23e96faf22f
2021-06-29 17:13:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 17:13:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 17:13:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 17:13:13 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 17:13:13 [scrapy.core.engine] INFO: Spider opened
2021-06-29 17:13:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 17:13:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 17:13:14 [project_crawler] INFO: æ•°æ®å¤„ç†å¼€å§‹
2021-06-29 17:13:14 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': '2021å¹´ä½ å¯èƒ½ä¸çŸ¥é“çš„ CSS ç‰¹æ€§ï¼ˆä¸‹ç¯‡ï¼‰', 'brief_content': 'åœ¨è¿™ä¸ªè¯é¢˜ä¸­ä¸»è¦æ•´ç†äº†æœ‰å…³äº CSS æ–¹é¢çš„ç‰¹æ€§ï¼Œå¹¶ä¸”å°½å¯èƒ½çš„æ•´ç†äº†ä¸€äº›å¤§å®¶ç°åœ¨èƒ½ç”¨æˆ–è¿‡ä¸äº†å¤šä¹…å°±èƒ½ç”¨çš„å±æ€§ã€‚å¦å¤–ï¼Œè™½ç„¶æ ‡é¢˜æ˜¯â€œæ–°ç‰¹æ€§â€ï¼Œä½†å…¶ä¸­æœ‰è›®å¤šç‰¹æ€§å¹¶ä¸æ˜¯â€œæ–°â€ï¼Œå¯èƒ½å·²ç»å‡ºç°åœ¨ä½ çš„é¡¹ç›®ä¸­', 'user_name': 'æ·˜ç³»å‰ç«¯å›¢é˜Ÿ', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'äº§å“ç»ç†ï¼šé¸¿è’™é‚£ä¸ªå¼€åœºåŠ¨ç”»æŒºå¸…çš„ ç»™å’±ä»¬é¡µé¢ä¹Ÿæ•´ä¸€ä¸ªå‘—', 'brief_content': 'æœ‰ä¸€å¤©å¼€ä¼šï¼Œäº§å“ç»ç†é—®ï¼šå¤§å®¶éƒ½å‡é¸¿è’™ç³»ç»Ÿäº†ä¹ˆï¼Ÿç´§æ¥ç€ä¸€ç¾¤äººç­”ï¼šæˆ‘ä»¬éƒ½ç”¨iPhoneâ€¦ å½“ç„¶å“ˆï¼Œæˆ‘è‡ªå·±ç”¨çš„æ˜¯å®‰å“ï¼Œä¸è¿‡ä¹Ÿä¸æ˜¯åä¸º(ç•™ä¸‹äº†æ²¡é’±çš„æ³ªæ°´)â€¦ å¬äº†ä»–è¿™ä¹ˆä¸€é—®æˆ‘è¿˜ä»¥ä¸ºè¿™æ˜¯è¦è®©æˆ‘ä»¬å¼€å‘é¸¿è’™åº”ç”¨äº†å‘¢', 'user_name': 'æ‰‹æ’•çº¢é»‘æ ‘', 'tags': [None, None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'ä¸‰åƒæ–‡å­—ï¼Œåªä¸ºå†™å¥½ä¸€ä¸ª Function.prototype.call', 'brief_content': 'Function.prototype.callï¼Œæ‰‹å†™ç³»åˆ—ï¼Œä¸‡æ–‡é¢è¯•ç³»åˆ—ï¼Œå¿…ä¼šç³»åˆ—å¿…åŒ…å«çš„å†…å®¹ï¼Œè¶³è§å…¶åœ¨å‰ç«¯çš„åˆ†é‡ã€‚ æœ¬æ–‡åŸºäºMDN å’Œ ECMA æ ‡å‡†ï¼Œå’Œå¤§å®¶ä¸€èµ·ä»æ–°è®¤è¯†callã€‚', 'user_name': 'äº‘çš„ä¸–ç•Œ', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'WOCï¼åŸæ¥ Linux ç»ˆç«¯ä¸‹å±…ç„¶è¿˜æœ‰è¿›ç¨‹è®°å¸åŠŸèƒ½ï¼Ÿï¼', 'brief_content': 'åœ¨ç³»ç»Ÿç®¡ç†ä¸­ï¼Œæœ‰æ—¶éœ€è¦è®°å½•ç”¨æˆ·å¯¹èµ„æºçš„æ¶ˆè´¹æƒ…å†µï¼Œä½œä¸ºå¯¹ç”¨æˆ·è´¦å·æ”¶å–è´¹ç”¨çš„ä¾æ®ã€‚è¿™äº›æ—¥å¿—ä¹Ÿå¯ä»¥ç”¨äºå®‰å…¨ç›®çš„ï¼Œæä¾›æœ‰å…³ç³»ç»Ÿæ´»åŠ¨çš„æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚', 'user_name': 'æ°å“¥çš„ITä¹‹æ—…', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'ä½¿ç”¨ vscode çœæ—¶çš„6ä¸ªæ’ä»¶', 'brief_content': 'ä½¿ç”¨ Visual Studio Code å¼€å‘é¡¹ç›®çœæ—¶çš„å…­ä¸ªæ’ä»¶ã€‚ 1. vscode-icon ä¸åŒçš„æ–‡ä»¶å±•ç¤ºä¸åŒçš„å›¾æ ‡ï¼Œæ–¹ä¾¿å¿«é€Ÿè¯†åˆ«æ–‡ä»¶ç±»å‹ã€‚ 2. Color Highlight é¢œè‰²æ ‡è®°ã€‚ ', 'user_name': 'Jimmy', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'ã€Œå¾®ä¿¡å°ç¨‹åºã€ç”Ÿæˆæ°´å°åŸç†ä¸æ’ä»¶ç¼–å†™', 'brief_content': 'ä¸€ å‰è¨€ ä»Šå¤©åˆ†äº«ä¸€ä¸ªå°ç¨‹åºç”Ÿæˆæ°´å°çš„å°æŠ€å·§â€”â€”canvasç»˜åˆ¶èƒŒæ™¯å›¾ï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šè¯¦ç»†ä»‹ç»ç»˜åˆ¶çš„ç»†èŠ‚ã€‚å¸Œæœ›å¼€å‘è¿‡å¾®ä¿¡å°ç¨‹åºçš„åŒå­¦å¯ä»¥æŠŠæ–‡ç« æ”¶è—èµ·æ¥ï¼Œè¿™æ ·å¦‚æœä»¥åé‡åˆ°ç±»ä¼¼çš„éœ€æ±‚ï¼Œå¯ä»¥ç¿»å‡ºæ¥ä½œä¸ºå‚è€ƒã€‚ æœ¬', 'user_name': 'æˆ‘ä¸æ˜¯å¤–æ˜Ÿäºº', 'tags': [None, None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'three.js å®ç°é£æš´äº‘ç‰¹æ•ˆ', 'brief_content': 'å¤§å®¶å¥½ï¼Œè¿™é‡Œæ˜¯ CSSå…¼WebGL é­”æ³•ä½¿â€”â€”alphardexã€‚ æœ¬æ–‡æˆ‘ä»¬å°†ç”¨three.jsæ¥å®ç°é£æš´äº‘ç‰¹æ•ˆï¼Œä¸€èµ·æ¥åˆ›ä½œå§ï¼', 'user_name': 'alphardex', 'tags': [None, None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'æ‰‹å†™ç³»åˆ—-å®ç°ä¸€ä¸ªé“‚é‡‘æ®µä½çš„ React', 'brief_content': 'æœ¬æ–‡å®ç°ç®€å•ç‰ˆæœ¬çš„ Reactï¼Œå‚è€ƒ React 16.8 çš„åŸºæœ¬åŠŸèƒ½ï¼ŒåŒ…æ‹¬è™šæ‹Ÿ DOMã€Fiberã€Diff ç®—æ³•ã€å‡½æ•°å¼ç»„ä»¶ã€hooks ç­‰ã€‚', 'user_name': 'æ¸…æ±¤é¥ºå­', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'æ¼«ç”» | å¦‚æœé¢è¯•æ—¶å¤§å®¶éƒ½è¯´çœŸè¯â€¦', 'brief_content': 'ç”»é¢è¿‡äºçœŸå®ï¼Œæ˜“å¼•èµ·ä¸é€‚è¯·æ…å…¥â€¦â€¦é¢è¯•é€ ç«ç®­ å…¥èŒæ‹§èºä¸ æ˜¯å½“å‰èŒåœºçš„ä¸­æœ€ç¾é£æ™¯çº¿ é¢è¯•æ—¶çš„é«˜æ ‡å‡†ã€ä¸¥è¦æ±‚ å®é™…å·¥ä½œå´æ˜¯ä¿®ä¿®è¡¥è¡¥æˆ–åšç€å¿™ç¢Œä½†æ— å…³ç´§è¦çš„äº‹æƒ… ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§ç°è±¡å‘¢ï¼Ÿ', 'user_name': 'è‹å—', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'èµ·è¯‰ä¹¦ï¼æ…•è¯¾ç½‘ä½ æ•¢æŠ„è¢­ï¼Œæˆ‘å°±æ•¢èµ·è¯‰ä½ ï¼', 'brief_content': 'è¿™å‘¨å°±å¼€å§‹èµ°èµ·è¯‰æµç¨‹å•¦ï¼æœ€è¿‘ä¹Ÿåœ¨çœ‹ã€Šä¸­å›½è‘—ä½œæƒæ³•ã€‹å’Œã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹ä¸­çš„ä¸€äº›æ³•å¾‹æ¡æ–‡ï¼Œæ­£å¸¸æƒ…å†µä¸‹ï¼Œä¸€ä¸ªç¨‹åºå‘˜ä¹Ÿä¸ä¼šå»çœ‹è¿™äº›æ³•å¾‹æ–‡ä»¶çš„ï¼Œä½†æ˜¯ç”Ÿæ´»ä¸­æ€»æœ‰æ„å¤–ã€‚', 'user_name': 'ç¨‹åºå‘˜åä¸‰', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'è¿˜åœ¨è¿Ÿç–‘æ˜¯å¦ä¸Štsï¼Ÿå…ˆä¸Šè½¦å†è¯´ï¼vue3+tså¼€å‘åˆä½“éªŒ', 'brief_content': 'æ–‡æœ¬ä¸»è¦ç»“åˆæ¡ˆä¾‹ä½“éªŒä¸€ä¸‹vue3+tså¼€å‘çš„å®é™…æ•ˆæœã€‚åˆ°åº•é€‚ä¸é€‚åˆä½ å’Œä½ çš„é¡¹ç›®ï¼Œè¿˜å¾—æ ¹æ®å„ä½çœ‹å®˜è‡ªå·±æŒæ¡ç¨‹åº¦å’Œé¡¹ç›®å®é™…æƒ…å†µç»¼åˆåˆ¤æ–­ã€‚', 'user_name': 'æ¨æ‘é•¿', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'ã€ŠBootstrap5é›¶åŸºç¡€åˆ°ç²¾é€šã€‹ç¬¬26èŠ‚ Bootstrap5æ¨¡æ€å¼¹æ¡†Modalç»„ä»¶ç”¨æ³•', 'brief_content': 'è¿™æ˜¯æˆ‘å‚ä¸æ›´æ–‡æŒ‘æˆ˜çš„ç¬¬26å¤©ï¼Œæ´»åŠ¨è¯¦æƒ…æŸ¥çœ‹ï¼š æ›´æ–‡æŒ‘æˆ˜ 26.1 Bootstrap5æ¨¡æ€å¼¹æ¡†å·¥ä½œåŸç† ä½¿ç”¨Bootstrapçš„JavaScriptæ¨¡å¼æ’ä»¶å°†å¯¹è¯æ¡†æ·»åŠ åˆ°ç«™ç‚¹ä¸­ï¼Œç”¨äºç¯ç®±ã€ç”¨æˆ·é€šçŸ¥æˆ–å®Œ', 'user_name': 'ä¿ºè€åˆ˜', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': '2K Starï¼è¶…è¿‡ 50 ä¸ªä¸“é¢˜ã€450 ä¸ªå¥½é¡¹ç›®ï¼Œå¤§åŠå¹´æ¥æ¨èè¿‡çš„é‡ç£…é¡¹ç›®åˆé›† ğŸ‘', 'brief_content': 'è¿™å¤§åŠå¹´æ¥ï¼ŒçŒ«å“¥å·²ç»æ¨èè¿‡è¶…è¿‡ 50ä¸ªä¸“é¢˜ï¼Œ 450 ä¸ªè¶…çº§å¥½çš„å¼€æºé¡¹ç›®äº†ï¼Œä»Šå¤©æŠŠå¾€æœŸæ¨èè¿‡çš„æ–‡ç« ä¸é¡¹ç›®åšä¸ªåˆé›†å§ï¼Œæ–¹ä¾¿å¤§å®¶èƒ½å¿«é€Ÿå¾—æŸ¥é˜…åˆ°æƒ³è¦çš„é¡¹ç›®ã€‚', 'user_name': 'å¤©æ˜å¤œå°½', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'ä¸‰å¤©ä¸‰å¤œï¼Œæ•´ç†äº†30å¼ é«˜æ¸…æ€ç»´å¯¼å›¾ | å¸¦ä½ é‡æ¸©ES6ï¼ˆä¸‹ï¼‰', 'brief_content': 'æœ€è¿‘é‡æ–°å­¦ä¹ ES6ï¼Œæˆ‘å°†æ ¸å¿ƒçŸ¥è¯†æ±‡æ€»æ•´ç†æˆè„‘å›¾~~å»ºè®®æ”¶è—ï¼Œè¯´ä¸å®šæŸå¤©è¦çœ‹ä¸€çœ¼ï¼ï¼~~~~~~~~~', 'user_name': 'LBJ', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'è§£æ”¾ç”Ÿäº§åŠ›ï¼Œè‡ªåŠ¨åŒ–ç”ŸæˆVueç»„ä»¶æ–‡æ¡£', 'brief_content': 'ä¸€ã€ç°çŠ¶ Vueæ¡†æ¶åœ¨å‰ç«¯å¼€å‘ä¸­åº”ç”¨å¹¿æ³›ï¼Œå½“ä¸€ä¸ªå¤šäººå¼€å‘çš„Vueé¡¹ç›®ç»è¿‡é•¿æœŸç»´æŠ¤ä¹‹åå¾€å¾€ä¼šæ²‰æ·€å‡ºå¾ˆå¤šçš„å…¬å…±ç»„ä»¶ï¼Œè¿™ä¸ªæ—¶å€™ç»å¸¸ä¼šå‡ºç°ä¸€ä¸ªäºº å¼€å‘äº†ä¸€ä¸ªç»„ä»¶è€Œå…¶ä»–ç»´æŠ¤è€…æˆ–æ–°æ¥æ‰‹çš„äººå´ä¸çŸ¥é“è¿™ä¸ªç»„ä»¶æ˜¯åšä»€ä¹ˆ', 'user_name': 'ä¸€åªèŠ±å–µ', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'ã€ŠBootstrap5é›¶åŸºç¡€åˆ°ç²¾é€šã€‹ç¬¬27èŠ‚ Bootstrap5å¼¹å‡ºæç¤ºå’Œå·¥å…·æç¤ºç»„ä»¶ç”¨æ³•', 'brief_content': 'è¿™æ˜¯æˆ‘å‚ä¸æ›´æ–‡æŒ‘æˆ˜çš„ç¬¬27å¤©ï¼Œæ´»åŠ¨è¯¦æƒ…æŸ¥çœ‹ï¼š æ›´æ–‡æŒ‘æˆ˜ 27.1 æ¦‚è¿° è¿™å‡ è¦è®²ä¸¤ä¸ªæ§ä»¶ï¼šå¼¹å‡ºæç¤ºï¼ˆPopoversï¼‰å’Œå·¥å…·æç¤ºï¼ˆTooltipsï¼‰ï¼Œè¿™ä¸¤ä¸ªç»„ä»¶åŠŸèƒ½éƒ½å¾ˆå•ä¸€ï¼Œç”¨æ³•ä¹Ÿå¾ˆç®€å•ï¼Œæœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹', 'user_name': 'ä¿ºè€åˆ˜', 'tags': [None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'æœ‰è¶£çš„ Kotlin 0x01ï¼šScala-like functions', 'brief_content': '0x01ï¼šScala-like functions ä»¥ä¸Šä»£ç ï¼Œè¿è¡Œç»“æœä¸ºä½•ï¼Ÿå¯é€‰é¡¹ï¼š Does not compile Prints "Hello, World" Nothing Something ', 'user_name': 'æ˜“å†¬', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'ä¸€ä¸ªé«˜æ€§èƒ½ã€å°è€Œç¾çš„åºåˆ—åŒ–å·¥å…·ï¼', 'brief_content': 'ä½œè€…ï¼šfredalxin\\ åœ°å€ï¼šhttps://fredal.xin/kryo-quickstart Kryoæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„åºåˆ—åŒ–/ååºåˆ—åŒ–å·¥å…·ï¼Œç”±äºå…¶å˜é•¿å­˜å‚¨ç‰¹æ€§å¹¶ä½¿ç”¨äº†å­—èŠ‚ç ç”Ÿæˆæœºåˆ¶ï¼Œæ‹¥æœ‰è¾ƒ', 'user_name': 'JavaæŠ€æœ¯æ ˆ', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': '14ä¸‡å­— | 400 å¤šé“ JavaScript é¢è¯•é¢˜ ğŸ“ æœ‰ç­”æ¡ˆ ğŸŒ (ç¬¬äºŒéƒ¨åˆ† 101-200é¢˜)', 'brief_content': 'è¿™æ˜¯æˆ‘å‚ä¸æ›´æ–‡æŒ‘æˆ˜çš„ç¬¬28å¤©ï¼Œæ´»åŠ¨è¯¦æƒ…æŸ¥çœ‹ï¼š æ›´æ–‡æŒ‘æˆ˜ 14ä¸‡å­— | 400 å¤šé“ JavaScript é¢è¯•é¢˜ ğŸ“ æœ‰ç­”æ¡ˆ ğŸŒ (ç¬¬äºŒéƒ¨åˆ† 101-200é¢˜)', 'user_name': 'æµ·æ‹¥', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [scrapy.core.scraper] ERROR: Error processing {'title': 'Django Rest Framework æºç è§£è¯»ï¼ˆ å…­ï¼‰åˆ†é¡µåŠŸèƒ½', 'brief_content': 'è¿™æ˜¯æˆ‘å‚ä¸æ›´æ–‡æŒ‘æˆ˜çš„ç¬¬18å¤©ï¼Œæ´»åŠ¨è¯¦æƒ…æŸ¥çœ‹ï¼š æ›´æ–‡æŒ‘æˆ˜ ä¸€ã€åˆ†é¡µä»‹ç» åˆ†é¡µæœ‰ä¸‰ç§æ–¹å¼ æ™®é€šåˆ†é¡µï¼Œçœ‹ç¬¬né¡µï¼Œæ¯é¡µæ˜¾ç¤ºmæ¡æ•°æ®ï¼› åˆ‡å‰²åˆ†é¡µï¼Œåœ¨nä¸ªä½ç½®ï¼Œå‘åæŸ¥çœ‹mæ¡æ•°æ®ï¼› åŠ å¯†åˆ†é¡µï¼Œè¿™ä¸æ™®é€šåˆ†é¡µæ–¹å¼ç›¸ä¼¼ï¼Œ', 'user_name': 'Honest1y', 'tags': [None, None]}
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\Python3.7.6\lib\site-packages\scrapy\utils\defer.py", line 157, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\pipelines.py", line 27, in process_item
    self.log("DBæ·»åŠ æ•°æ®", logging.INFO)
AttributeError: 'CrawlerPipeline' object has no attribute 'log'
2021-06-29 17:13:14 [project_crawler] INFO: æ•°æ®å¤„ç†ç»“æŸ
2021-06-29 17:13:14 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 17:13:14 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\JsonExporterPipleline.py", line 22, in close_spider
    self.log("çˆ¬è™«ç»“æŸ", logging.INFO)
AttributeError: 'JsonExporterPipleline' object has no attribute 'log'
2021-06-29 17:13:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 478,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 15521,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 1.178848,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 9, 13, 14, 998338),
 'log_count/ERROR': 24,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 9, 13, 13, 819490)}
2021-06-29 17:13:15 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 17:18:04 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 17:18:04 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 17:18:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 17:18:04 [scrapy.extensions.telnet] INFO: Telnet Password: 1c05228b25b61dbb
2021-06-29 17:18:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 17:18:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 17:18:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 17:18:04 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 17:18:04 [scrapy.core.engine] INFO: Spider opened
2021-06-29 17:18:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 17:18:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 17:18:05 [project_crawler] INFO: æ•°æ®å¤„ç†å¼€å§‹
2021-06-29 17:18:05 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:18:05 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:18:05 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:18:05 [project_crawler] INFO: æ•°æ®å¤„ç†ç»“æŸ
2021-06-29 17:18:05 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 17:18:05 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\JsonExporterPipleline.py", line 22, in close_spider
    self.log("çˆ¬è™«ç»“æŸ", logging.INFO)
AttributeError: 'JsonExporterPipleline' object has no attribute 'log'
2021-06-29 17:18:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 478,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 15678,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.836761,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 9, 18, 5, 649987),
 'item_scraped_count': 20,
 'log_count/ERROR': 4,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 9, 18, 4, 813226)}
2021-06-29 17:18:05 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 17:20:45 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 17:20:45 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 17:20:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 17:20:45 [scrapy.extensions.telnet] INFO: Telnet Password: ba124921e6b13bb6
2021-06-29 17:20:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 17:20:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 17:20:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 17:20:46 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 17:20:46 [scrapy.core.engine] INFO: Spider opened
2021-06-29 17:20:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 17:20:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 17:20:48 [project_crawler] INFO: æ•°æ®å¤„ç†å¼€å§‹
2021-06-29 17:20:48 [project_crawler] INFO: æ•°æ®å¤„ç†ç»“æŸ
2021-06-29 17:20:48 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 17:20:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 492,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 9030,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 1.778244,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 9, 20, 48, 621811),
 'item_scraped_count': 10,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 9, 20, 46, 843567)}
2021-06-29 17:20:48 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 17:23:12 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 17:23:12 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 17:23:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 17:23:12 [scrapy.extensions.telnet] INFO: Telnet Password: dd4c64659e5d5457
2021-06-29 17:23:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 17:23:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 17:23:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 17:23:13 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 17:23:13 [scrapy.core.engine] INFO: Spider opened
2021-06-29 17:23:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 17:23:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 17:23:13 [project_crawler] INFO: æ•°æ®å¤„ç†å¼€å§‹
2021-06-29 17:23:13 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:24:41 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2021-06-29 17:24:41 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:24:41 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:24:41 [project_crawler] INFO: æ•°æ®å¤„ç†ç»“æŸ
2021-06-29 17:24:41 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 17:24:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 478,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 15770,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 88.499743,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 9, 24, 41, 656278),
 'item_scraped_count': 20,
 'log_count/ERROR': 3,
 'log_count/INFO': 13,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 9, 23, 13, 156535)}
2021-06-29 17:24:41 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 17:25:11 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 17:25:11 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 17:25:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 17:25:11 [scrapy.extensions.telnet] INFO: Telnet Password: 9c0c93e738e73db8
2021-06-29 17:25:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 17:25:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 17:25:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 17:25:12 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 17:25:12 [scrapy.core.engine] INFO: Spider opened
2021-06-29 17:25:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 17:25:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 17:25:13 [project_crawler] INFO: æ•°æ®å¤„ç†å¼€å§‹
2021-06-29 17:25:13 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:25:13 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:25:13 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:25:13 [project_crawler] INFO: æ•°æ®å¤„ç†ç»“æŸ
2021-06-29 17:25:13 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 17:25:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 478,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 15685,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 1.008305,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 9, 25, 13, 934844),
 'item_scraped_count': 20,
 'log_count/ERROR': 3,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 9, 25, 12, 926539)}
2021-06-29 17:25:13 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 17:25:14 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 17:25:14 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 17:25:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 17:25:14 [scrapy.extensions.telnet] INFO: Telnet Password: 1c1f629f54eb9a53
2021-06-29 17:25:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 17:25:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 17:25:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 17:25:15 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 17:25:15 [scrapy.core.engine] INFO: Spider opened
2021-06-29 17:25:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 17:25:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 17:25:16 [project_crawler] INFO: æ•°æ®å¤„ç†å¼€å§‹
2021-06-29 17:25:16 [project_crawler] INFO: æ•°æ®å¤„ç†ç»“æŸ
2021-06-29 17:25:16 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 17:25:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 492,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 8970,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 1.233701,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 9, 25, 16, 708426),
 'item_scraped_count': 10,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 9, 25, 15, 474725)}
2021-06-29 17:25:16 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 17:26:30 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 17:26:30 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 17:26:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 17:26:30 [scrapy.extensions.telnet] INFO: Telnet Password: e0b5d33a016855b2
2021-06-29 17:26:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 17:26:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 17:26:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 17:26:30 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 17:26:30 [scrapy.core.engine] INFO: Spider opened
2021-06-29 17:26:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 17:26:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 17:26:31 [project_crawler] INFO: æ•°æ®å¤„ç†å¼€å§‹
2021-06-29 17:26:31 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:26:31 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:26:31 [project_crawler] ERROR: æ•°æ®å¤„ç†å¤±è´¥,å¤±è´¥åŸå› 'NoneType' object has no attribute 'get'
2021-06-29 17:26:31 [project_crawler] INFO: æ•°æ®å¤„ç†ç»“æŸ
2021-06-29 17:26:31 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 17:26:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 478,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 15784,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.835763,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 9, 26, 31, 486681),
 'item_scraped_count': 20,
 'log_count/ERROR': 3,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 9, 26, 30, 650918)}
2021-06-29 17:26:31 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 20:18:36 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 20:18:36 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 20:18:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 20:18:36 [scrapy.extensions.telnet] INFO: Telnet Password: 119555aea4437df8
2021-06-29 20:18:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 20:18:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 20:18:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 20:18:37 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 20:18:37 [scrapy.core.engine] INFO: Spider opened
2021-06-29 20:18:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:18:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 20:18:47 [project_crawler] INFO: <twisted.python.failure.Failure scrapy.spidermiddlewares.httperror.HttpError: Ignoring non-200 response>
2021-06-29 20:18:47 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 20:18:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 380,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 687,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 9.905539,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 12, 18, 47, 817567),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 12, 18, 37, 912028)}
2021-06-29 20:18:47 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 20:26:10 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 20:26:10 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 20:26:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 20:26:10 [scrapy.extensions.telnet] INFO: Telnet Password: d460378498deccfe
2021-06-29 20:26:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 20:26:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 20:26:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 20:26:11 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 20:26:11 [scrapy.core.engine] INFO: Spider opened
2021-06-29 20:26:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:26:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 20:26:11 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\scrapy\core\engine.py", line 129, in _next_request
    request = next(slot.start_requests)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\spiders\project_crawler.py", line 40, in start_requests
    param += val+"&&"
TypeError: unsupported operand type(s) for +: 'int' and 'str'
2021-06-29 20:26:11 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 20:26:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.058842,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 12, 26, 11, 272400),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2021, 6, 29, 12, 26, 11, 213558)}
2021-06-29 20:26:11 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 20:29:28 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 20:29:28 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 20:29:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 20:29:28 [scrapy.extensions.telnet] INFO: Telnet Password: 1f8e3627c6976f07
2021-06-29 20:29:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 20:29:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 20:29:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 20:29:28 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 20:29:28 [scrapy.core.engine] INFO: Spider opened
2021-06-29 20:29:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:29:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 20:29:28 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\scrapy\core\engine.py", line 129, in _next_request
    request = next(slot.start_requests)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\spiders\project_crawler.py", line 40, in start_requests
    param = param + val+"&&"
TypeError: can only concatenate str (not "int") to str
2021-06-29 20:29:28 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 20:29:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.030917,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 12, 29, 28, 982185),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2021, 6, 29, 12, 29, 28, 951268)}
2021-06-29 20:29:28 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 20:30:13 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 20:30:13 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 20:30:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 20:30:14 [scrapy.extensions.telnet] INFO: Telnet Password: 64446a8364c5b51f
2021-06-29 20:30:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 20:30:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 20:30:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 20:30:14 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 20:30:14 [scrapy.core.engine] INFO: Spider opened
2021-06-29 20:30:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:30:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 20:31:24 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\scrapy\core\engine.py", line 129, in _next_request
    request = next(slot.start_requests)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\spiders\project_crawler.py", line 40, in start_requests
    param = param +param_item['keys']+'='+ val+"&&"
TypeError: can only concatenate str (not "int") to str
2021-06-29 20:31:24 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 20:31:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 70.289331,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 12, 31, 24, 805106),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2021, 6, 29, 12, 30, 14, 515775)}
2021-06-29 20:31:24 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 20:31:50 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 20:31:50 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 20:31:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 20:31:50 [scrapy.extensions.telnet] INFO: Telnet Password: e294ec9f47f69be3
2021-06-29 20:31:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 20:31:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 20:31:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 20:31:51 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 20:31:51 [scrapy.core.engine] INFO: Spider opened
2021-06-29 20:31:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:31:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 20:31:56 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\scrapy\core\engine.py", line 129, in _next_request
    request = next(slot.start_requests)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\spiders\project_crawler.py", line 40, in start_requests
    param = param + param_item['keys']+'=' + val+"&&"
TypeError: can only concatenate str (not "int") to str
2021-06-29 20:31:56 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 20:31:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 5.009612,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 12, 31, 56, 34162),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2021, 6, 29, 12, 31, 51, 24550)}
2021-06-29 20:31:56 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 20:32:27 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 20:32:27 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 20:32:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 20:32:27 [scrapy.extensions.telnet] INFO: Telnet Password: 8f25c843332454b7
2021-06-29 20:32:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 20:32:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 20:32:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 20:32:28 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 20:32:28 [scrapy.core.engine] INFO: Spider opened
2021-06-29 20:32:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:32:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 20:33:09 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\scrapy\core\engine.py", line 129, in _next_request
    request = next(slot.start_requests)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\spiders\project_crawler.py", line 40, in start_requests
    param = param + param_item['keys']+'=' + val+"&&"
TypeError: can only concatenate str (not "int") to str
2021-06-29 20:33:09 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 20:33:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 41.690371,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 12, 33, 9, 797084),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2021, 6, 29, 12, 32, 28, 106713)}
2021-06-29 20:33:09 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 20:33:28 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 20:33:28 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 20:33:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 20:33:28 [scrapy.extensions.telnet] INFO: Telnet Password: 20f3e7955a084f80
2021-06-29 20:33:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 20:33:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 20:33:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 20:33:29 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 20:33:29 [scrapy.core.engine] INFO: Spider opened
2021-06-29 20:33:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:33:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 20:34:37 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "D:\Python3.7.6\lib\site-packages\scrapy\core\engine.py", line 129, in _next_request
    request = next(slot.start_requests)
  File "E:\python\payapa\WebCrawler\api\application\crawler\crawler\spiders\project_crawler.py", line 40, in start_requests
    param = param + param_item['keys']+'=' + val+"&&"
TypeError: can only concatenate str (not "int") to str
2021-06-29 20:34:37 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 20:34:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 68.358128,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 12, 34, 37, 492068),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'start_time': datetime.datetime(2021, 6, 29, 12, 33, 29, 133940)}
2021-06-29 20:34:37 [scrapy.core.engine] INFO: Spider closed (finished)
2021-06-29 20:35:14 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: crawler)
2021-06-29 20:35:14 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17134-SP0
2021-06-29 20:35:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'crawler',
 'LOG_FILE': '../log/2021-6-29.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'crawler.spiders',
 'SPIDER_MODULES': ['crawler.spiders']}
2021-06-29 20:35:14 [scrapy.extensions.telnet] INFO: Telnet Password: d6d1821d9bd9bce8
2021-06-29 20:35:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2021-06-29 20:35:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-06-29 20:35:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-06-29 20:35:14 [scrapy.middleware] INFO: Enabled item pipelines:
['crawler.pipelines.CrawlerPipeline',
 'crawler.JsonExporterPipleline.JsonExporterPipleline']
2021-06-29 20:35:14 [scrapy.core.engine] INFO: Spider opened
2021-06-29 20:35:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:35:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-06-29 20:36:54 [project_crawler] INFO: <twisted.python.failure.Failure scrapy.spidermiddlewares.httperror.HttpError: Ignoring non-200 response>
2021-06-29 20:36:54 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2021-06-29 20:36:54 [scrapy.core.engine] INFO: Closing spider (finished)
2021-06-29 20:36:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 487,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 685,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 99.815696,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 29, 12, 36, 54, 723217),
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 29, 12, 35, 14, 907521)}
2021-06-29 20:36:54 [scrapy.core.engine] INFO: Spider closed (finished)
